{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Fmu62uRTx-"
      },
      "source": [
        "## Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qTG4af2RIg1",
        "outputId": "97ff2b08-3894-4cae-e058-6260cbc445ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gpustat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdiffeq\n",
            "  Downloading torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from torchdiffeq) (2.0.0+cu118)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from torchdiffeq) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy>=1.4.0->torchdiffeq) (1.22.4)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (3.11.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.3.0->torchdiffeq) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.3.0->torchdiffeq) (16.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.3.0->torchdiffeq) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.3.0->torchdiffeq) (1.3.0)\n",
            "Installing collected packages: torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install -q flwr[simulation] torch torchvision matplotlib\n",
        "!pip install torchdiffeq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pNvHqcWYsNc"
      },
      "source": [
        "**Download the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3asgevYYqg1",
        "outputId": "f9a7bcc3-2455-4b92-cfd4-28374802cc60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Pems_Dataset'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 44 (delta 9), reused 41 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (44/44), 31.72 MiB | 8.60 MiB/s, done.\n",
            "/content/Pems_Dataset\n",
            "models\tPEMS04\tpems04_dtw_distance.npy  pems04_spatial_distance.npy  README.md\n"
          ]
        }
      ],
      "source": [
        "# !rm -r Pems_Dataset/\n",
        "!git clone https://github.com/ebagirma/Pems_Dataset.git\n",
        "%cd Pems_Dataset\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttiXANStRfNa",
        "outputId": "f6c98c44-7d7d-4e1f-a813-1aa23c4963ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cuda using PyTorch 1.13.1+cu116 and Flower 1.3.0\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from Pems_Dataset.models.model import ODEGCN\n",
        "# from Pems_Dataset.models.Update import LocalUpdate\n",
        "from torch.optim.lr_scheduler import StepLR, OneCycleLR\n",
        "\n",
        "import copy\n",
        "import argparse\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "from fastdtw import fastdtw\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "import flwr as fl\n",
        "\n",
        "DEVICE = torch.device(\"cuda\")  # Try \"cuda\" to train on GPU\n",
        "print(\n",
        "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk9Ok0aUVrcS"
      },
      "source": [
        "# Load arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpEU7TpSVueb"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--remote', action='store_true', help='the code run on a server')\n",
        "parser.add_argument('--num-gpu', type=int, default=0, help='the number of the gpu to use')\n",
        "parser.add_argument('--epochs', type=int, default=1, help='train epochs')\n",
        "parser.add_argument('--batch-size', type=int, default=16, help='batch size')\n",
        "parser.add_argument('--batch', type=int, default=16, help='batch size')\n",
        "\n",
        "\n",
        "parser.add_argument('--frac', type=float, default=0.01, help=\"the fraction of clients: C\")\n",
        "parser.add_argument('--num_users', type=int, default=100)\n",
        "\n",
        "\n",
        "parser.add_argument('--filename', type=str, default='pems04')\n",
        "parser.add_argument('--train-ratio', type=float, default=0.6, help='the ratio of training dataset')\n",
        "parser.add_argument('--valid-ratio', type=float, default=0.2, help='the ratio of validating dataset')\n",
        "parser.add_argument('--his-length', type=int, default=12, help='the length of history time series of input')\n",
        "parser.add_argument('--pred-length', type=int, default=12, help='the length of target time series for prediction')\n",
        "\n",
        "parser.add_argument('--sigma1', type=float, default=0.1, help='sigma for the semantic matrix')\n",
        "parser.add_argument('--sigma2', type=float, default=10, help='sigma for the spatial matrix')\n",
        "parser.add_argument('--thres1', type=float, default=0.6, help='the threshold for the semantic matrix')\n",
        "parser.add_argument('--thres2', type=float, default=0.5, help='the threshold for the spatial matrix')\n",
        "parser.add_argument('--lr', type=float, default=2e-3, help='learning rate')\n",
        "\n",
        "parser.add_argument('--log', action='store_true', help='if write log to files')\n",
        "args, unkown = parser.parse_known_args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQNUnhubRsTN"
      },
      "outputs": [],
      "source": [
        "files = {\n",
        "    'pems03': ['PEMS03/pems03.npz', 'PEMS03/distance.csv'],\n",
        "    'pems04': ['PEMS04/PEMS04.npz', 'PEMS04/distance.csv'],\n",
        "    'pems07': ['PEMS07/pems07.npz', 'PEMS07/distance.csv'],\n",
        "    'pems08': ['PEMS08/pems08.npz', 'PEMS08/distance.csv'],\n",
        "    'pemsbay': ['PEMSBAY/pems_bay.npz', 'PEMSBAY/distance.csv'],\n",
        "    'pemsD7M': ['PeMSD7M/PeMSD7M.npz', 'PeMSD7M/distance.csv'],\n",
        "    'pemsD7L': ['PeMSD7L/PeMSD7L.npz', 'PeMSD7L/distance.csv']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frIHuup_Vm7C"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWZfqih7UK8-",
        "outputId": "37a27f66-18af-49b7-80c1-92e19faf02a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "def read_data(args):\n",
        "    \"\"\"read data, generate spatial adjacency matrix and semantic adjacency matrix by dtw\n",
        "    Args:\n",
        "        sigma1: float, default=0.1, sigma for the semantic matrix\n",
        "        sigma2: float, default=10, sigma for the spatial matrix\n",
        "        thres1: float, default=0.6, the threshold for the semantic matrix\n",
        "        thres2: float, default=0.5, the threshold for the spatial matrix\n",
        "    Returns:\n",
        "        data: tensor, T * N * 1\n",
        "        dtw_matrix: array, semantic adjacency matrix\n",
        "        sp_matrix: array, spatial adjacency matrix\n",
        "    \"\"\"\n",
        "    filename = args.filename\n",
        "    file = files[filename]\n",
        "    filepath = \"./Pems_Dataset/\"\n",
        "    if args.remote:\n",
        "        filepath = './Pems_Dataset/'\n",
        "    data = np.load(filepath + file[0])['data']\n",
        "    # PEMS04 == shape: (16992, 307, 3)    feature: flow,occupy,speed\n",
        "    # PEMSD7M == shape: (12672, 228, 1)\n",
        "    # PEMSD7L == shape: (12672, 1026, 1)\n",
        "    num_node = data.shape[1]\n",
        "    mean_value = np.mean(data, axis=(0, 1)).reshape(1, 1, -1)\n",
        "    std_value = np.std(data, axis=(0, 1)).reshape(1, 1, -1)\n",
        "    data = (data - mean_value) / std_value\n",
        "    mean_value = mean_value.reshape(-1)[0]\n",
        "    std_value = std_value.reshape(-1)[0]\n",
        "\n",
        "    if not os.path.exists(f'Pems_Dataset/{filename}_dtw_distance.npy'):\n",
        "        data_mean = np.mean([data[:, :, 0][24*12*i: 24*12*(i+1)] for i in range(data.shape[0]//(24*12))], axis=0)\n",
        "        data_mean = data_mean.squeeze().T \n",
        "        dtw_distance = np.zeros((num_node, num_node))\n",
        "        for i in tqdm(range(num_node)):\n",
        "            for j in range(i, num_node):\n",
        "                dtw_distance[i][j] = fastdtw(data_mean[i], data_mean[j], radius=6)[0]\n",
        "        for i in range(num_node):\n",
        "            for j in range(i):\n",
        "                dtw_distance[i][j] = dtw_distance[j][i]\n",
        "        np.save(f'data/{filename}_dtw_distance.npy', dtw_distance)\n",
        "\n",
        "    dist_matrix = np.load(f'Pems_Dataset/{filename}_dtw_distance.npy')\n",
        "\n",
        "    mean = np.mean(dist_matrix)\n",
        "    std = np.std(dist_matrix)\n",
        "    dist_matrix = (dist_matrix - mean) / std\n",
        "    sigma = args.sigma1\n",
        "    dist_matrix = np.exp(-dist_matrix ** 2 / sigma ** 2)\n",
        "    dtw_matrix = np.zeros_like(dist_matrix)\n",
        "    dtw_matrix[dist_matrix > args.thres1] = 1\n",
        "\n",
        "    # use continuous spatial matrix\n",
        "    if not os.path.exists(f'Pems_Dataset/{filename}_spatial_distance.npy'):\n",
        "        with open(filepath + file[1], 'r') as fp:\n",
        "            dist_matrix = np.zeros((num_node, num_node)) + np.float('inf')\n",
        "            file = csv.reader(fp)\n",
        "            for line in file:\n",
        "                break\n",
        "            for line in file:\n",
        "                start = int(line[0])\n",
        "                end = int(line[1])\n",
        "                dist_matrix[start][end] = float(line[2])\n",
        "                dist_matrix[end][start] = float(line[2])\n",
        "            np.save(f'Pems_Dataset/{filename}_spatial_distance.npy', dist_matrix)\n",
        "\n",
        "\n",
        "    dist_matrix = np.load(f'Pems_Dataset/{filename}_spatial_distance.npy')\n",
        "    # normalization\n",
        "    std = np.std(dist_matrix[dist_matrix != np.float('inf')])\n",
        "    mean = np.mean(dist_matrix[dist_matrix != np.float('inf')])\n",
        "    dist_matrix = (dist_matrix - mean) / std\n",
        "    sigma = args.sigma2\n",
        "    sp_matrix = np.exp(- dist_matrix**2 / sigma**2)\n",
        "    sp_matrix[sp_matrix < args.thres2] = 0 \n",
        "\n",
        "\n",
        "    print(f'average degree of spatial graph is {np.sum(sp_matrix > 0)/2/num_node}')\n",
        "    print(f'average degree of semantic graph is {np.sum(dtw_matrix > 0)/2/num_node}')\n",
        "    return torch.from_numpy(data.astype(np.float32)), mean_value, std_value, dtw_matrix, sp_matrix\n",
        "\n",
        "\n",
        "def get_normalized_adj(A):\n",
        "    \"\"\"\n",
        "    Returns a tensor, the degree normalized adjacency matrix.\n",
        "    \"\"\"\n",
        "    alpha = 0.8\n",
        "    D = np.array(np.sum(A, axis=1)).reshape((-1,))\n",
        "    D[D <= 10e-5] = 10e-5    # Prevent infs\n",
        "    diag = np.reciprocal(np.sqrt(D))\n",
        "    A_wave = np.multiply(np.multiply(diag.reshape((-1, 1)), A),\n",
        "                         diag.reshape((1, -1)))\n",
        "    A_reg = alpha / 2 * (np.eye(A.shape[0]) + A_wave)\n",
        "    return torch.from_numpy(A_reg.astype(np.float32))\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, split_start, split_end, his_length, pred_length):\n",
        "        split_start = int(split_start)\n",
        "        split_end = int(split_end)\n",
        "        self.data = data[split_start: split_end]\n",
        "        self.his_length = his_length\n",
        "        self.pred_length = pred_length\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index: index + self.his_length].permute(1, 0, 2)\n",
        "        y = self.data[index + self.his_length: index + self.his_length + self.pred_length][:, :, 0].permute(1, 0)\n",
        "        return torch.Tensor(x), torch.Tensor(y)\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0] - self.his_length - self.pred_length + 1\n",
        "\n",
        "\n",
        "def generate_dataset(data, args):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        data: input dataset, shape like T * N\n",
        "        batch_size: int \n",
        "        train_ratio: float, the ratio of the dataset for training\n",
        "        his_length: the input length of time series for prediction\n",
        "        pred_length: the target length of time series of prediction\n",
        "    Returns:\n",
        "        train_dataloader: torch tensor, shape like batch * N * his_length * features\n",
        "        test_dataloader: torch tensor, shape like batch * N * pred_length * features\n",
        "    \"\"\"\n",
        "    batch_size = args.batch_size\n",
        "    train_ratio = args.train_ratio\n",
        "    valid_ratio = args.valid_ratio\n",
        "    his_length = args.his_length\n",
        "    pred_length = args.pred_length\n",
        "    train_dataset = MyDataset(data, 0, data.shape[0] * train_ratio, his_length, pred_length)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    valid_dataset = MyDataset(data, data.shape[0]*train_ratio, data.shape[0]*(train_ratio+valid_ratio), his_length, pred_length)\n",
        "    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    test_dataset = MyDataset(data, data.shape[0]*(train_ratio+valid_ratio), data.shape[0], his_length, pred_length)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return train_dataloader, valid_dataloader, test_dataloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p304xmctVWl2"
      },
      "source": [
        "# Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u9o3wkzVUYL",
        "outputId": "47aaec7d-9948-4409-956b-5528d36f221f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average degree of spatial graph is 1.1009771986970684\n",
            "average degree of semantic graph is 6.267100977198697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-04a0e5f37a81>:69: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  std = np.std(dist_matrix[dist_matrix != np.float('inf')])\n",
            "<ipython-input-6-04a0e5f37a81>:70: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  mean = np.mean(dist_matrix[dist_matrix != np.float('inf')])\n"
          ]
        }
      ],
      "source": [
        "data, mean, std, dtw_matrix, sp_matrix = read_data(args)\n",
        "train_dataset, valloaders, testloader = generate_dataset(data, args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io6NRN4rDKjL"
      },
      "source": [
        "**Hyper-parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAdTolezKqa7"
      },
      "outputs": [],
      "source": [
        "# Normalize Adjacency matrix\n",
        "A_sp_wave = get_normalized_adj(sp_matrix).to(DEVICE)\n",
        "A_se_wave = get_normalized_adj(dtw_matrix).to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be7owK3_YA-A"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cc00lPoACwYw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def divide_iid(dataset, num_users):\n",
        "    \"\"\"\n",
        "    Sample I.I.D. client data from PEMS03 dataset\n",
        "    :param dataset:\n",
        "    :param num_users:\n",
        "    :return: dict of data index \n",
        "    \"\"\"\n",
        "    \n",
        "    # I.e, here the length of the training dataset is 10172 and\n",
        "    # the number of user is 100, so the each dict user will have 10172/100 => 101\n",
        "    # 101 will be a the length of the dataset, which is dived for the 100 local users. \n",
        "    \n",
        "    num_items = int(len(dataset)/num_users)\n",
        "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
        "    for i in range(num_users):\n",
        "        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace=False))\n",
        "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
        "    return dict_users\n",
        "    \n",
        "dict_users = divide_iid(train_dataset, args.num_users)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZM9mPWZYUfw"
      },
      "outputs": [],
      "source": [
        "net_glob = ODEGCN(num_nodes=data.shape[1], \n",
        "                  num_features=data.shape[2], \n",
        "                  num_timesteps_input=args.his_length, \n",
        "                  num_timesteps_output=args.pred_length, \n",
        "                  A_sp_hat=A_sp_wave, \n",
        "                  A_se_hat=A_se_wave)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m88Bep2pC8U3"
      },
      "outputs": [],
      "source": [
        "lr = args.lr\n",
        "optimizer = torch.optim.AdamW(net_glob.parameters(), lr=lr)\n",
        "scheduler = StepLR(optimizer, step_size=50, gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-lykzmeSOBI"
      },
      "outputs": [],
      "source": [
        "# Set the model to train and send it to device.\n",
        "net_glob = net_glob.to(DEVICE)\n",
        "net_glob.train()\n",
        "\n",
        "\n",
        "# copy weights\n",
        "w_glob = net_glob.state_dict() #Get network parameters\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDrmuO_wBbfJ"
      },
      "outputs": [],
      "source": [
        "# training \n",
        "loss_train = []\n",
        "acc_train = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_MHifPjBqki"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "\n",
        "def FedAvg(w):\n",
        "    w_avg = copy.deepcopy(w[0])\n",
        "    for k in w_avg.keys():\n",
        "        for i in range(1, len(w)):\n",
        "            w_avg[k] += w[i][k]\n",
        "        w_avg[k] = torch.div(w_avg[k], len(w))\n",
        "    return w_avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j949DUluDN4e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, autograd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DatasetSplit(Dataset):\n",
        "    \"\"\"\n",
        "    Splits the datasets by the idxs\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, idxs):\n",
        "        self.dataset = dataset\n",
        "        self.idxs = list(idxs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idxs)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        input, label = self.dataset[self.idxs[item]]\n",
        "        return input, label\n",
        "\n",
        "\n",
        "class LocalUpdate(object):\n",
        "    \"\"\"\n",
        "    Model Aggregation (Federated Averaging)\n",
        "    \"\"\"\n",
        "    def __init__(self, args, dataset=None, idxs=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            args: contains arguments passeds\n",
        "            dataset: the training dataset \n",
        "            idxs: index of the dict users\n",
        "        \"\"\"\n",
        "        self.args = args\n",
        "        self.loss_func = nn.SmoothL1Loss()                         # It is less sensitive to outliers and prevents exploding gradients \n",
        "        self.selected_clients = []\n",
        "        self.ldr_train = DataLoader(DatasetSplit(dataset, idxs), batch_size=self.args.batch, shuffle=True)\n",
        "        print(type( self.ldr_train))\n",
        "    def train(self, net):\n",
        "        net.train()\n",
        "        # train and update\n",
        "        optimizer = torch.optim.SGD(net.parameters(), lr=self.args.lr, momentum=0.5)\n",
        "\n",
        "        epoch_loss = []\n",
        "        for iter in range(self.args.epochs):\n",
        "            \n",
        "            batch_loss = []\n",
        "            for batch_idx, (inputs, labels) in enumerate(self.ldr_train):\n",
        "                \n",
        "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "                \n",
        "                net.zero_grad()                                   #  Sets the gradients of all its parameters to zero for the local paramaters to learn new values\n",
        "                log_probs = net(inputs)\n",
        "                \n",
        "                # print(inputs.shape)\n",
        "                # print(log_probs.shape)\n",
        "                \n",
        "                loss = self.loss_func(log_probs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                \n",
        "                ## -------------------------- Prints the steps in each epoch ---------------------------- #\n",
        "                \n",
        "                print('Update Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    iter, batch_idx * len(inputs), len(self.ldr_train.dataset),\n",
        "                            100. * batch_idx / len(self.ldr_train), loss.item()))\n",
        "                \n",
        "                \n",
        "                batch_loss.append(loss.item())\n",
        "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
        "            \n",
        "        return net.state_dict(), sum(epoch_loss) / len(epoch_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EItGQ_QHiq48"
      },
      "outputs": [],
      "source": [
        "\n",
        "def mask_np(array, null_val):\n",
        "    if np.isnan(null_val):\n",
        "        return (~np.isnan(null_val)).astype('float32')\n",
        "    else:\n",
        "        return np.not_equal(array, null_val).astype('float32')\n",
        "\n",
        "\n",
        "def masked_mape_np(y_true, y_pred, null_val=np.nan):\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        mask = mask_np(y_true, null_val)\n",
        "        mask /= mask.mean()\n",
        "        mape = np.abs((y_pred - y_true) / y_true)\n",
        "        mape = np.nan_to_num(mask * mape)\n",
        "        return np.mean(mape) * 100\n",
        "\n",
        "\n",
        "def masked_rmse_np(y_true, y_pred, null_val=np.nan):\n",
        "    mask = mask_np(y_true, null_val)\n",
        "    mask /= mask.mean()\n",
        "    mse = (y_true - y_pred) ** 2\n",
        "    return np.sqrt(np.mean(np.nan_to_num(mask * mse)))\n",
        "\n",
        "\n",
        "def masked_mae_np(y_true, y_pred, null_val=np.nan):\n",
        "    mask = mask_np(y_true, null_val)\n",
        "    mask /= mask.mean()\n",
        "    mae = np.abs(y_true - y_pred)\n",
        "    return np.mean(np.nan_to_num(mask * mae))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBF0izPrg4zH"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def eval(loader, model, std, mean, device):\n",
        "    batch_rmse_loss = 0  \n",
        "    batch_mae_loss = 0\n",
        "    batch_mape_loss = 0\n",
        "    for idx, (inputs, targets) in enumerate(tqdm(loader)):\n",
        "        model.eval()\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        output = model(inputs)\n",
        "        \n",
        "        out_unnorm = output.detach().cpu().numpy()*std + mean\n",
        "        target_unnorm = targets.detach().cpu().numpy()*std + mean\n",
        "\n",
        "        mae_loss = masked_mae_np(target_unnorm, out_unnorm, 0)\n",
        "        rmse_loss = masked_rmse_np(target_unnorm, out_unnorm, 0)\n",
        "        mape_loss = masked_mape_np(target_unnorm, out_unnorm, 0)\n",
        "        batch_rmse_loss += rmse_loss\n",
        "        batch_mae_loss += mae_loss\n",
        "        batch_mape_loss += mape_loss\n",
        "\n",
        "    return batch_rmse_loss / (idx + 1), batch_mae_loss / (idx + 1), batch_mape_loss / (idx + 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Z_7OYuxm7Ua"
      },
      "outputs": [],
      "source": [
        "def mape(y_pred, y_true):\n",
        "    return torch.mean(torch.abs((y_true - y_pred) / y_true)) * 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LghjvMBmap5"
      },
      "outputs": [],
      "source": [
        "def train_eval(dataset, model, std, mean, device):\n",
        "    model.eval()\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=64) # Use a DataLoader with batch size of 64\n",
        "    criterion = nn.MSELoss()\n",
        "    rmse_losses, mae_losses, mape_losses = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred = model(X)\n",
        "            y_pred = (y_pred * std) + mean\n",
        "            y = (y * std) + mean\n",
        "            rmse_loss = torch.sqrt(criterion(y_pred, y))\n",
        "            mae_loss = nn.L1Loss()(y_pred, y)\n",
        "            mape_loss = mape(y_pred, y) # Check if the `mape()` function is implemented efficiently, as it may cause a bottleneck\n",
        "            rmse_losses.append(rmse_loss.item())\n",
        "            mae_losses.append(mae_loss.item())\n",
        "            mape_losses.append(mape_loss.item())\n",
        "    return np.mean(rmse_losses), np.mean(mae_losses), np.mean(mape_losses)\n",
        "\n",
        "\n",
        "def valid_eval(dataset, model, std, mean, device):\n",
        "    model.eval()\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=64) # Use a DataLoader with batch size of 64\n",
        "    criterion = nn.MSELoss()\n",
        "    rmse_losses, mae_losses, mape_losses = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred = model(X)\n",
        "            y_pred = (y_pred * std) + mean\n",
        "            y = (y * std) + mean\n",
        "            rmse_loss = torch.sqrt(criterion(y_pred, y))\n",
        "            mae_loss = nn.L1Loss()(y_pred, y)\n",
        "            mape_loss = mape(y_pred, y) # Check if the `mape()` function is implemented efficiently, as it may cause a bottleneck\n",
        "            rmse_losses.append(rmse_loss.item())\n",
        "            mae_losses.append(mae_loss.item())\n",
        "            mape_losses.append(mape_loss.item())\n",
        "    return np.mean(rmse_losses), np.mean(mae_losses), np.mean(mape_losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jiBHJ_am5_k"
      },
      "outputs": [],
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def test_net(loader, model, std, mean, device, args):\n",
        "    batch_rmse_loss = 0  \n",
        "    batch_mae_loss = 0\n",
        "    batch_mape_loss = 0\n",
        "    data_loader = DataLoader(loader, batch_size=args.batch)\n",
        "    for idx, (inputs, targets) in enumerate(tqdm(data_loader)):\n",
        "        model.eval()\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        output = model(inputs)\n",
        "        \n",
        "        out_unnorm = output.detach().cpu().numpy()*std + mean\n",
        "        target_unnorm = targets.detach().cpu().numpy()*std + mean\n",
        "         \n",
        "        mae_loss = masked_mae_np(target_unnorm, out_unnorm, 0)\n",
        "        rmse_loss = masked_rmse_np(target_unnorm, out_unnorm, 0)\n",
        "        mape_loss = masked_mape_np(target_unnorm, out_unnorm, 0)\n",
        "        \n",
        "        batch_rmse_loss += rmse_loss\n",
        "        batch_mae_loss += mae_loss\n",
        "        batch_mape_loss += mape_loss\n",
        "\n",
        "    return batch_rmse_loss / (idx + 1), batch_mae_loss / (idx + 1), batch_mape_loss / (idx + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1E0gxvMxBhFG",
        "outputId": "c8680883-8474-4942-c748-fd7f5b467d16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n",
            "<class 'numpy.int64'>\n",
            "<class 'torch.utils.data.dataloader.DataLoader'>\n",
            "Update Epoch: 0 [0/6 (0%)]\tLoss: 0.472551\n",
            "*********************Round   0, Average loss 0.473******************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        }
      ],
      "source": [
        "for iter in range(args.epochs):\n",
        "        \n",
        "        w_locals, loss_locals = [], []\n",
        "        m = max(int(args.frac * args.num_users), 1)                                              # Sets the max limit for the idx_user\n",
        "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)                   # Randomly choices value\n",
        "        \n",
        "        # print(len(idxs_users))\n",
        "        # print(idxs_users)\n",
        "        \n",
        "        for idx in idxs_users:\n",
        "            print('Training...')\n",
        "            print(type(idx))\n",
        "            local = LocalUpdate(args=args, dataset=train_dataset.dataset, idxs=list(dict_users[idx]))\n",
        "            net = copy.deepcopy(net_glob).to(DEVICE)\n",
        "\n",
        "\n",
        "            w, loss = local.train(net)\n",
        "            w_locals.append(copy.deepcopy(w))\n",
        "            loss_locals.append(copy.deepcopy(loss))\n",
        "\n",
        "            # print('Calling eval function...')\n",
        "            # train_rmse, train_mae, train_mape = train_eval(train_dataset.dataset, net, std, mean, DEVICE)\n",
        "            # valid_rmse, valid_mae, valid_mape = valid_eval(valloaders.dataset, net, std, mean, DEVICE)\n",
        "\n",
        "\n",
        "            # if valid_rmse < best_valid_rmse:\n",
        "            #   best_valid_rmse = valid_rmse\n",
        "            #   print('New best results!')\n",
        "            #   torch.save(net.state_dict(), f'net_params_{args.filename}_{args.num_gpu}.pkl')\n",
        "\n",
        "\n",
        "            # print(f'\\n##on train data## loss: {loss}, \\n' + \n",
        "            #                 f'##on train data## rmse loss: {train_rmse}, mae loss: {train_mae}, mape loss: {train_mape}\\n' +\n",
        "            #                 f'##on valid data## rmse loss: {valid_rmse}, mae loss: {valid_mae}, mape loss: {valid_mape}\\n')\n",
        "                    \n",
        "\n",
        "        scheduler.step()         # Decays the learning rate of each parameter group by gamma=0.5 every step_size epochs.\n",
        "\n",
        "        # update global weights\n",
        "        w_glob = FedAvg(w_locals)\n",
        "\n",
        "        # copy weight to net_glob\n",
        "        net_glob.load_state_dict(w_glob)\n",
        "\n",
        "        # print loss\n",
        "        loss_avg = sum(loss_locals) / len(loss_locals)\n",
        "        print('*********************Round {:3d}, Average loss {:.3f}******************************'.format(iter, loss_avg))\n",
        "        loss_train.append(loss_avg)\n",
        "        acc_train.append(1-loss_avg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCYs07d0CMIw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80043abf-e4e1-444d-dc01-f96848b53830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 211/211 [00:16<00:00, 13.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##on test data## rmse loss: 184.6001821942804, mae loss: 149.38064332934917, mape loss: 209.61457233858334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test_rmse, test_mae, test_mape = test_net(testloader.dataset, net_glob, std, mean, DEVICE, args)\n",
        "print(f'##on test data## rmse loss: {test_rmse}, mae loss: {test_mae}, mape loss: {test_mape}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tZfy4uIy5q1D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}