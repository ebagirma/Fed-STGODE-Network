{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing dependencies"
      ],
      "metadata": {
        "id": "16Fmu62uRTx-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qTG4af2RIg1",
        "outputId": "4e37999e-e563-4a64-c471-11a10c181608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: flwr-nightly in /usr/local/lib/python3.9/dist-packages (1.4.0.dev20230420)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.9/dist-packages (from flwr-nightly) (0.0.2)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.19.0 in /usr/local/lib/python3.9/dist-packages (from flwr-nightly) (3.20.3)\n",
            "Requirement already satisfied: grpcio!=1.52.0,<2.0.0,>=1.48.2 in /usr/local/lib/python3.9/dist-packages (from flwr-nightly) (1.53.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.0 in /usr/local/lib/python3.9/dist-packages (from flwr-nightly) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: flwr-nightly[simulation] in /usr/local/lib/python3.9/dist-packages (1.4.0.dev20230420)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.0 in /usr/local/lib/python3.9/dist-packages (from flwr-nightly[simulation]) (1.22.4)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.19.0 in /usr/local/lib/python3.9/dist-packages (from flwr-nightly[simulation]) (3.20.3)\n",
            "Requirement already satisfied: grpcio!=1.52.0,<2.0.0,>=1.48.2 in /usr/local/lib/python3.9/dist-packages (from flwr-nightly[simulation]) (1.53.0)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.9/dist-packages (from flwr-nightly[simulation]) (0.0.2)\n",
            "Requirement already satisfied: ray[default]<3.0.0,>=2.3.0 in /usr/local/lib/python3.9/dist-packages (from flwr-nightly[simulation]) (2.3.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.9/dist-packages (from ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (4.3.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (6.0)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.9/dist-packages (from ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (1.3.3)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (1.0.5)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.9/dist-packages (from ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (1.3.1)\n",
            "Requirement already satisfied: virtualenv>=20.0.24 in /usr/local/lib/python3.9/dist-packages (from ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (20.22.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (2.27.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.9/dist-packages (from ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (23.1.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (8.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (3.11.0)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.9/dist-packages (from ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (0.7.0)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.9/dist-packages (from ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (3.8.4)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (0.16.0)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.9/dist-packages (from ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (0.5.5)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (0.3.14)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (1.10.7)\n",
            "Requirement already satisfied: gpustat>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (1.1)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.9/dist-packages (from ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (0.11.2)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.9/dist-packages (from ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (6.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp>=3.7->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp>=3.7->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp>=3.7->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp>=3.7->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (2.0.12)\n",
            "Requirement already satisfied: blessed>=1.17.1 in /usr/local/lib/python3.9/dist-packages (from gpustat>=1.0.0->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (1.20.0)\n",
            "Requirement already satisfied: psutil>=5.6.0 in /usr/local/lib/python3.9/dist-packages (from gpustat>=1.0.0->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (5.9.5)\n",
            "Requirement already satisfied: nvidia-ml-py>=11.450.129 in /usr/local/lib/python3.9/dist-packages (from gpustat>=1.0.0->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (11.525.112)\n",
            "Requirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.9/dist-packages (from virtualenv>=20.0.24->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (3.2.0)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.9/dist-packages (from virtualenv>=20.0.24->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (0.3.6)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (0.19.3)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.9/dist-packages (from opencensus->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from opencensus->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (2.11.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (3.4)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.9/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (0.2.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (1.16.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.14.1 in /usr/local/lib/python3.9/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (2.17.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.9/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (1.59.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<3.0.0,>=2.3.0->flwr-nightly[simulation]) (0.4.8)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchdiffeq in /usr/local/lib/python3.9/dist-packages (0.2.3)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from torchdiffeq) (1.10.1)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from torchdiffeq) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy>=1.4.0->torchdiffeq) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (3.11.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->torchdiffeq) (3.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.3.0->torchdiffeq) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.3.0->torchdiffeq) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.3.0->torchdiffeq) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.3.0->torchdiffeq) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fastdtw in /usr/local/lib/python3.9/dist-packages (0.3.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from fastdtw) (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -U flwr-nightly\n",
        "!pip3 install -U flwr-nightly[simulation]\n",
        "!pip install torchdiffeq\n",
        "!pip install fastdtw"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download the dataset**"
      ],
      "metadata": {
        "id": "5pNvHqcWYsNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r Pems_Dataset/\n",
        "!git clone https://github.com/ebagirma/Pems_Dataset.git\n",
        "%cd Pems_Dataset\n",
        "!ls"
      ],
      "metadata": {
        "id": "B3asgevYYqg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c347f60-9743-475e-dc76-3fa180812f8c"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Pems_Dataset' already exists and is not an empty directory.\n",
            "/content/Pems_Dataset\n",
            "models\tPEMS04\tpems04_dtw_distance.npy  pems04_spatial_distance.npy  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "from typing import List, Tuple\n",
        "\n",
        "from flwr.common import Metrics\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "from Pems_Dataset.models.model import ODEGCN\n",
        "# from utils import MyDataset, read_data, get_normalized_adj\n",
        "# from eval import masked_mae_np, masked_mape_np, masked_rmse_np\n",
        "\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "import ray\n",
        "import flwr as fl\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import csv\n",
        "\n",
        "from fastdtw import fastdtw\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch"
      ],
      "metadata": {
        "id": "ttiXANStRfNa"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda. is_available():\n",
        "    print(\"Using Cuda!\")\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"Using CPU\")\n",
        "    DEVICE = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "6tYHoG2RZns9",
        "outputId": "43aadaa3-6bb2-4709-9068-42c994fee2ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Cuda!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load arguments"
      ],
      "metadata": {
        "id": "Uk9Ok0aUVrcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.remote = False\n",
        "        self.num_gpu = 0\n",
        "        self.global_epochs = 5\n",
        "        self.local_epoch = 10\n",
        "        self.batch_size = 16\n",
        "        self.batch = 16\n",
        "        self.frac = 0.1\n",
        "        self.num_users = 100\n",
        "        self.filename = 'pems04'\n",
        "        self.train_ratio = 0.6\n",
        "        self.valid_ratio = 0.2\n",
        "        self.his_length = 12\n",
        "        self.pred_length = 12\n",
        "        self.sigma1 = 0.1\n",
        "        self.sigma2 = 10\n",
        "        self.thres1 = 0.6\n",
        "        self.thres2 = 0.5\n",
        "        self.lr = 2e-3\n",
        "        self.log = False\n",
        "\n",
        "args = Args()"
      ],
      "metadata": {
        "id": "qpEU7TpSVueb"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = {\n",
        "    'pems03': ['PEMS03/pems03.npz', 'PEMS03/distance.csv'],\n",
        "    'pems04': ['PEMS04/PEMS04.npz', 'PEMS04/distance.csv'],\n",
        "    'pems07': ['PEMS07/pems07.npz', 'PEMS07/distance.csv'],\n",
        "    'pems08': ['PEMS08/pems08.npz', 'PEMS08/distance.csv'],\n",
        "    'pemsbay': ['PEMSBAY/pems_bay.npz', 'PEMSBAY/distance.csv'],\n",
        "    'pemsD7M': ['PeMSD7M/PeMSD7M.npz', 'PeMSD7M/distance.csv'],\n",
        "    'pemsD7L': ['PeMSD7L/PeMSD7L.npz', 'PeMSD7L/distance.csv']\n",
        "}"
      ],
      "metadata": {
        "id": "tQNUnhubRsTN"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "frIHuup_Vm7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "\n",
        "def read_data(args):\n",
        "    \"\"\"read data, generate spatial adjacency matrix and semantic adjacency matrix by dtw\n",
        "\n",
        "    Args:\n",
        "        sigma1: float, default=0.1, sigma for the semantic matrix\n",
        "        sigma2: float, default=10, sigma for the spatial matrix\n",
        "        thres1: float, default=0.6, the threshold for the semantic matrix\n",
        "        thres2: float, default=0.5, the threshold for the spatial matrix\n",
        "\n",
        "    Returns:\n",
        "        data: tensor, T * N * 1\n",
        "        dtw_matrix: array, semantic adjacency matrix\n",
        "        sp_matrix: array, spatial adjacency matrix\n",
        "    \"\"\"\n",
        "    filename = args.filename\n",
        "    file = files[filename]\n",
        "    filepath = \"./Pems_Dataset/\"\n",
        "    if args.remote:\n",
        "        filepath = './Pems_Dataset/'\n",
        "\n",
        "\n",
        "        \n",
        "    data = np.load(filepath + file[0])['data']\n",
        "    # PEMS04 == shape: (16992, 307, 3)    feature: flow,occupy,speed\n",
        "    # PEMSD7M == shape: (12672, 228, 1)\n",
        "    # PEMSD7L == shape: (12672, 1026, 1)\n",
        "    num_node = data.shape[1]\n",
        "    mean_value = np.mean(data, axis=(0, 1)).reshape(1, 1, -1)\n",
        "    std_value = np.std(data, axis=(0, 1)).reshape(1, 1, -1)\n",
        "    data = (data - mean_value) / std_value\n",
        "    mean_value = mean_value.reshape(-1)[0]\n",
        "    std_value = std_value.reshape(-1)[0]\n",
        "\n",
        "    if not os.path.exists(f'Pems_Dataset/{filename}_dtw_distance.npy'):\n",
        "        data_mean = np.mean([data[:, :, 0][24*12*i: 24*12*(i+1)] for i in range(data.shape[0]//(24*12))], axis=0)\n",
        "        data_mean = data_mean.squeeze().T \n",
        "        dtw_distance = np.zeros((num_node, num_node))\n",
        "        for i in tqdm(range(num_node)):\n",
        "            for j in range(i, num_node):\n",
        "                dtw_distance[i][j] = fastdtw(data_mean[i], data_mean[j], radius=6)[0]\n",
        "        for i in range(num_node):\n",
        "            for j in range(i):\n",
        "                dtw_distance[i][j] = dtw_distance[j][i]\n",
        "        np.save(f'data/{filename}_dtw_distance.npy', dtw_distance)\n",
        "\n",
        "    dist_matrix = np.load(f'Pems_Dataset/{filename}_dtw_distance.npy')\n",
        "\n",
        "    mean = np.mean(dist_matrix)\n",
        "    std = np.std(dist_matrix)\n",
        "    dist_matrix = (dist_matrix - mean) / std\n",
        "    sigma = args.sigma1\n",
        "    dist_matrix = np.exp(-dist_matrix ** 2 / sigma ** 2)\n",
        "    dtw_matrix = np.zeros_like(dist_matrix)\n",
        "    dtw_matrix[dist_matrix > args.thres1] = 1\n",
        "\n",
        "    # # use continuous semantic matrix\n",
        "    # if not os.path.exists(f'data/{filename}_dtw_c_matrix.npy'):\n",
        "    #     dist_matrix = np.load(f'data/{filename}_dtw_distance.npy')\n",
        "    #     # normalization\n",
        "    #     std = np.std(dist_matrix[dist_matrix != np.float('inf')])\n",
        "    #     mean = np.mean(dist_matrix[dist_matrix != np.float('inf')])\n",
        "    #     dist_matrix = (dist_matrix - mean) / std\n",
        "    #     sigma = 0.1\n",
        "    #     dtw_matrix = np.exp(- dist_matrix**2 / sigma**2)\n",
        "    #     dtw_matrix[dtw_matrix < 0.5] = 0 \n",
        "    #     np.save(f'data/{filename}_dtw_c_matrix.npy', dtw_matrix)\n",
        "    # dtw_matrix = np.load(f'data/{filename}_dtw_c_matrix.npy')\n",
        "    \n",
        "    # use continuous spatial matrix\n",
        "    if not os.path.exists(f'Pems_Dataset/{filename}_spatial_distance.npy'):\n",
        "        with open(filepath + file[1], 'r') as fp:\n",
        "            dist_matrix = np.zeros((num_node, num_node)) + np.float('inf')\n",
        "            file = csv.reader(fp)\n",
        "            for line in file:\n",
        "                break\n",
        "            for line in file:\n",
        "                start = int(line[0])\n",
        "                end = int(line[1])\n",
        "                dist_matrix[start][end] = float(line[2])\n",
        "                dist_matrix[end][start] = float(line[2])\n",
        "            np.save(f'Pems_Dataset/{filename}_spatial_distance.npy', dist_matrix)\n",
        "\n",
        "\n",
        "\n",
        "    dist_matrix = np.load(f'Pems_Dataset/{filename}_spatial_distance.npy')\n",
        "    # normalization\n",
        "    std = np.std(dist_matrix[dist_matrix != np.inf])\n",
        "    mean = np.mean(dist_matrix[dist_matrix != np.inf])\n",
        "    dist_matrix = (dist_matrix - mean) / std\n",
        "    sigma = args.sigma2\n",
        "    sp_matrix = np.exp(- dist_matrix**2 / sigma**2)\n",
        "    sp_matrix[sp_matrix < args.thres2] = 0 \n",
        " \n",
        "\n",
        "    print(f'average degree of spatial graph is {np.sum(sp_matrix > 0)/2/num_node}')\n",
        "    print(f'average degree of semantic graph is {np.sum(dtw_matrix > 0)/2/num_node}')\n",
        "    return torch.from_numpy(data.astype(np.float32)), mean_value, std_value, dtw_matrix, sp_matrix\n",
        "\n",
        "\n",
        "def get_normalized_adj(A):\n",
        "    \"\"\"\n",
        "    Returns a tensor, the degree normalized adjacency matrix.\n",
        "    \"\"\"\n",
        "    alpha = 0.8\n",
        "    D = np.array(np.sum(A, axis=1)).reshape((-1,))\n",
        "    D[D <= 10e-5] = 10e-5    # Prevent infs\n",
        "    diag = np.reciprocal(np.sqrt(D))\n",
        "    A_wave = np.multiply(np.multiply(diag.reshape((-1, 1)), A),\n",
        "                         diag.reshape((1, -1)))\n",
        "    A_reg = alpha / 2 * (np.eye(A.shape[0]) + A_wave)\n",
        "    return torch.from_numpy(A_reg.astype(np.float32))\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, split_start, split_end, his_length, pred_length):\n",
        "        split_start = int(split_start)\n",
        "        split_end = int(split_end)\n",
        "        self.data = data[split_start: split_end]\n",
        "        self.his_length = his_length\n",
        "        self.pred_length = pred_length\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index: index + self.his_length].permute(1, 0, 2)\n",
        "        y = self.data[index + self.his_length: index + self.his_length + self.pred_length][:, :, 0].permute(1, 0)\n",
        "        return torch.Tensor(x), torch.Tensor(y)\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0] - self.his_length - self.pred_length + 1\n",
        "\n",
        "\n",
        "def generate_dataset(data, args):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        data: input dataset, shape like T * N\n",
        "        batch_size: int \n",
        "        train_ratio: float, the ratio of the dataset for training\n",
        "        his_length: the input length of time series for prediction\n",
        "        pred_length: the target length of time series of prediction\n",
        "\n",
        "    Returns:\n",
        "        train_dataloader: torch tensor, shape like batch * N * his_length * features\n",
        "        test_dataloader: torch tensor, shape like batch * N * pred_length * features\n",
        "    \"\"\"\n",
        "    batch_size = args.batch_size\n",
        "    train_ratio = args.train_ratio\n",
        "    valid_ratio = args.valid_ratio\n",
        "    his_length = args.his_length\n",
        "    pred_length = args.pred_length\n",
        "    train_dataset = MyDataset(data, 0, data.shape[0] * train_ratio, his_length, pred_length)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    valid_dataset = MyDataset(data, data.shape[0]*train_ratio, data.shape[0]*(train_ratio+valid_ratio), his_length, pred_length)\n",
        "    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    test_dataset = MyDataset(data, data.shape[0]*(train_ratio+valid_ratio), data.shape[0], his_length, pred_length)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return train_dataloader, valid_dataloader, test_dataloader\n"
      ],
      "metadata": {
        "id": "FWZfqih7UK8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "712fdab0-2591-41af-8751-5f0effe99431"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, split_start, split_end, his_length, pred_length):\n",
        "        split_start = int(split_start)\n",
        "        split_end = int(split_end)\n",
        "        self.data = data[split_start: split_end]\n",
        "        self.his_length = his_length\n",
        "        self.pred_length = pred_length\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index: index + self.his_length].permute(1, 0, 2)\n",
        "        y = self.data[index + self.his_length: index + self.his_length + self.pred_length][:, :, 0].permute(1, 0)\n",
        "        return torch.Tensor(x), torch.Tensor(y)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.data.shape[0] - self.his_length - self.pred_length + 1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RdcQaiv1MldX"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_parameters(net) -> List[np.ndarray]:\n",
        "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
        "\n",
        "def set_parameters(net, parameters: List[np.ndarray]):\n",
        "    params_dict = zip(net.state_dict().keys(), parameters)\n",
        "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "    #print('works till here', len(state_dict))\n",
        "    net.load_state_dict(state_dict, strict=True)"
      ],
      "metadata": {
        "id": "-QwWSqweZUrk"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_np(array, null_val):\n",
        "    if np.isnan(null_val):\n",
        "        return (~np.isnan(null_val)).astype('float32')\n",
        "    else:\n",
        "        return np.not_equal(array, null_val).astype('float32')\n",
        "\n",
        "\n",
        "def masked_mape_np(y_true, y_pred, null_val=np.nan):\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        mask = mask_np(y_true, null_val)\n",
        "        mask /= mask.mean()\n",
        "        mape = np.abs((y_pred - y_true) / y_true)\n",
        "        mape = np.nan_to_num(mask * mape)\n",
        "        return np.mean(mape) * 100\n",
        "\n",
        "\n",
        "def masked_rmse_np(y_true, y_pred, null_val=np.nan):\n",
        "    mask = mask_np(y_true, null_val)\n",
        "    mask /= mask.mean()\n",
        "    mse = (y_true - y_pred) ** 2\n",
        "    return np.sqrt(np.mean(np.nan_to_num(mask * mse)))\n",
        "\n",
        "\n",
        "def masked_mae_np(y_true, y_pred, null_val=np.nan):\n",
        "    mask = mask_np(y_true, null_val)\n",
        "    mask /= mask.mean()\n",
        "    mae = np.abs(y_true - y_pred)\n",
        "    return np.mean(np.nan_to_num(mask * mae))\n"
      ],
      "metadata": {
        "id": "n6a-IeNSZ8cs"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(loader, model, optimizer, criterion, epochs, scheduler, std, mean,device):\n",
        "    print('training for epochs:', epochs)\n",
        "    epoch_loss = []\n",
        "    model.train()\n",
        "    for epoch in range(1, epochs+1):\n",
        "        print('epoch#:', epoch)\n",
        "        batch_loss = 0\n",
        "        for idx, (inputs, targets) in enumerate(loader):\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_loss += loss.detach().cpu().item() \n",
        "        epoch_loss.append(batch_loss)\n",
        "        loss = batch_loss\n",
        "       \n",
        "        train_rmse, train_mae, train_mape = eval(loader, model, std, mean, device)\n",
        "\n",
        "        if args.log:\n",
        "            logger.info(f'\\n##on train data## loss: {loss}, \\n' + \n",
        "                        f'##on train data## rmse loss: {train_rmse}, mae loss: {train_mae}, mape loss: {train_mape}\\n')\n",
        "        else:\n",
        "            print(f'\\n##on train data## loss: {loss}, \\n' + \n",
        "                f'##on train data## rmse loss: {train_rmse}, mae loss: {train_mae}, mape loss: {train_mape}\\n')\n",
        "        \n",
        "    scheduler.step()\n",
        "\n",
        "    return sum(epoch_loss)/epochs"
      ],
      "metadata": {
        "id": "pHO31v5_ZSIE"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def eval(loader, model, std, mean, device):\n",
        "    batch_rmse_loss = 0  \n",
        "    batch_mae_loss = 0\n",
        "    batch_mape_loss = 0\n",
        "    print('evaluating')\n",
        "    model.eval()\n",
        "    for idx, (inputs, targets) in enumerate(loader):\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        output = model(inputs)\n",
        "        \n",
        "        out_unnorm = output.detach().cpu().numpy()*std + mean\n",
        "        target_unnorm = targets.detach().cpu().numpy()*std + mean\n",
        "\n",
        "        mae_loss = masked_mae_np(target_unnorm, out_unnorm, 0)\n",
        "        rmse_loss = masked_rmse_np(target_unnorm, out_unnorm, 0)\n",
        "        mape_loss = masked_mape_np(target_unnorm, out_unnorm, 0)\n",
        "        batch_rmse_loss += rmse_loss\n",
        "        batch_mae_loss += mae_loss\n",
        "        batch_mape_loss += mape_loss\n",
        "    print(\"eval rmse loss: \", batch_rmse_loss/ (idx + 1))\n",
        "\n",
        "    return batch_rmse_loss / (idx + 1), batch_mae_loss / (idx + 1), batch_mape_loss / (idx + 1)\n"
      ],
      "metadata": {
        "id": "omAq816GZOgC"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset(data, args):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        data: input dataset, shape like T * N\n",
        "        batch_size: int \n",
        "        train_ratio: float, the ratio of the dataset for training\n",
        "        his_length: the input length of time series for prediction\n",
        "        pred_length: the target length of time series of prediction\n",
        "\n",
        "    Returns:\n",
        "        train_dataloader: torch tensor, shape like batch * N * his_length * features\n",
        "        test_dataloader: torch tensor, shape like batch * N * pred_length * features\n",
        "    \"\"\"\n",
        "    batch_size = args.batch_size\n",
        "    train_ratio = args.train_ratio\n",
        "    valid_ratio = args.valid_ratio\n",
        "    his_length = args.his_length\n",
        "    pred_length = args.pred_length\n",
        "    train_dataset = MyDataset(data, 0, data.shape[0] * train_ratio, his_length, pred_length)\n",
        "\n",
        "    valid_dataset = MyDataset(data, data.shape[0]*train_ratio, data.shape[0]*(train_ratio+valid_ratio), his_length, pred_length)\n",
        "\n",
        "    test_dataset = MyDataset(data, data.shape[0]*(train_ratio+valid_ratio), data.shape[0], his_length, pred_length)\n",
        "\n",
        "    return train_dataset, valid_dataset, test_dataset\n"
      ],
      "metadata": {
        "id": "ETW-XIt2X7Bi"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Federated Learning with Flower**"
      ],
      "metadata": {
        "id": "o_Shp2xHkLo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, cid, net, trainloader, valloader, optimiser, schedular, learning_rate, epochs, loss_function, mean, std ):\n",
        "        self.cid = cid\n",
        "        self.net = net\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "        self.loss_function = loss_function\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.optimiser = torch.optim.AdamW(net.parameters(), lr=self.learning_rate)\n",
        "        self.scheduler = StepLR(self.optimiser, step_size=50, gamma=0.5)\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return get_parameters(self.net)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        set_parameters(self.net, parameters)\n",
        "        loss = train(self.trainloader, self.net, self.optimiser ,  self.loss_function, self.epochs , self.scheduler, self.std, self.mean, DEVICE)\n",
        "        return get_parameters(self.net), len(self.trainloader)*self.trainloader.batch_size, {\"train_loss\":loss}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        set_parameters(self.net, parameters)\n",
        "        valid_rmse, valid_mae, valid_mape = eval(self.valloader, self.net, self.std, self.mean, DEVICE)\n",
        "        return valid_rmse, len(self.valloader)*self.valloader.batch_size , {'valid_mape':valid_mape, 'valid_mae':valid_mae, 'valid_rmse':valid_rmse}\n",
        "    \n"
      ],
      "metadata": {
        "id": "3pS0ZyX0A4xl"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def client_fn(cid: str) -> FlowerClient:\n",
        "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
        "\n",
        "    epochs = 2\n",
        "    # Load model\n",
        "    learning_rate = lr\n",
        "    net = ODEGCN(num_nodes=data.shape[1], \n",
        "            num_features=data.shape[2], \n",
        "            num_timesteps_input=args.his_length, \n",
        "            num_timesteps_output=args.pred_length, \n",
        "            A_sp_hat=A_sp_wave, \n",
        "            A_se_hat=A_se_wave).to(DEVICE)\n",
        "    #net.std = std\n",
        "    #net.mean = mean\n",
        "\n",
        "    w_glob = net.state_dict()\n",
        "    \n",
        "    net.load_state_dict(w_glob, strict=True)\n",
        "    net(torch.rand(args.batch_size, feature_size, 12, 3).to(DEVICE))\n",
        "    # will train and evaluate on their own unique data\n",
        "    trainloader = trainloaders[int(cid)]\n",
        "    valloader = valloaders[int(cid)]\n",
        "\n",
        "    # Create a  single Flower client representing a single organization\n",
        "    return FlowerClient(cid = cid, net=net, trainloader=trainloader, valloader=valloader, optimiser=None , schedular=None,learning_rate=learning_rate, \n",
        "                        epochs=epochs , loss_function= criterion , mean=mean, std=std)"
      ],
      "metadata": {
        "id": "p_mmrGyrHlbO"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
        "    loss = [num_examples * m[\"train_loss\"] for num_examples, m in metrics]\n",
        "    loss_client = [m[\"train_loss\"] for num_examples, m in metrics]\n",
        "    examples = [num_examples for num_examples, _ in metrics]\n",
        "    return {'client_metrics': {\"loss\":loss_client}, 'examples':examples,'average_metrics':{'average loss': sum(loss)/sum(examples)}}\n",
        "    "
      ],
      "metadata": {
        "id": "_qFj29A3fEOA"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
        "    # Multiply accuracy of each client by number of examples used\n",
        "\n",
        "    valid_rmse = [num_examples * m[\"valid_rmse\"] for num_examples, m in metrics]\n",
        "    valid_mae = [num_examples * m[\"valid_mae\"] for num_examples, m in metrics]\n",
        "    valid_mape = [num_examples * m[\"valid_mape\"] for num_examples, m in metrics]\n",
        "    examples = [num_examples for num_examples, _ in metrics]\n",
        "\n",
        "    valid_rmse_client = [m[\"valid_rmse\"] for num_examples, m in metrics]\n",
        "    valid_mae_client = [m[\"valid_mae\"] for num_examples, m in metrics]\n",
        "    valid_mape_client = [m[\"valid_mape\"] for num_examples, m in metrics]\n",
        "\n",
        "\n",
        "    # Aggregate and return custom metric (weighted average)\n",
        "    return {'average_metrics':{'valid_mape':sum(valid_mape)/sum(examples), 'valid_mae':sum(valid_mae)/sum(examples), 'valid_rmse':sum(valid_rmse)/sum(examples)},\n",
        "            'client_metrics': {'valid_mape':valid_mape_client, 'valid_mae':valid_mae_client, 'valid_rmse':valid_rmse_client, 'examples': examples}}\n"
      ],
      "metadata": {
        "id": "sU12CJmPA4vE"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if args.log:\n",
        "    logger.add('log_{time}.log')\n",
        "options = vars(args)\n",
        "if args.log:\n",
        "    logger.info(options)\n",
        "else:\n",
        "    print(options)"
      ],
      "metadata": {
        "id": "qQBjG9U4uBsW",
        "outputId": "661f6c3d-5feb-4907-d467-859ebd4f653d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'remote': False, 'num_gpu': 0, 'global_epochs': 5, 'local_epoch': 10, 'batch_size': 16, 'batch': 16, 'frac': 0.1, 'num_users': 100, 'filename': 'pems04', 'train_ratio': 0.6, 'valid_ratio': 0.2, 'his_length': 12, 'pred_length': 12, 'sigma1': 0.1, 'sigma2': 10, 'thres1': 0.6, 'thres2': 0.5, 'lr': 0.002, 'log': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = args.local_epoch\n",
        "NUM_CLIENTS = 5\n",
        "!pwd\n",
        "data, mean, std, dtw_matrix, sp_matrix = read_data(args)\n",
        "train_loader, valid_loader, test_loader = generate_dataset(data, args)\n",
        "A_sp_wave = get_normalized_adj(sp_matrix).to(DEVICE)\n",
        "A_se_wave = get_normalized_adj(dtw_matrix).to(DEVICE)"
      ],
      "metadata": {
        "id": "7cy7Uc4Abxmh",
        "outputId": "32bc5b89-69bd-4e1f-f4a5-9af9326a87d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "average degree of spatial graph is 1.1009771986970684\n",
            "average degree of semantic graph is 6.267100977198697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = args.batch_size\n",
        "train_loader = Subset(train_loader, np.arange(1000))\n",
        "valid_loader = Subset(valid_loader, np.arange(200))\n",
        "feature_size = train_loader.dataset.data.shape[1]\n",
        "inds = np.array_split(np.random.randint(len(train_loader), size=len(train_loader)), NUM_CLIENTS)\n",
        "trainloaders = []\n",
        "valloaders = []\n",
        "for idx in inds:\n",
        "    trainloaders.append(DataLoader(Subset(train_loader, idx), batch_size=batch_size, shuffle=True))\n",
        "\n",
        "\n",
        "inds = np.array_split(np.random.randint(len(valid_loader), size=len(valid_loader)), NUM_CLIENTS)\n",
        "for idx in inds:\n",
        "    valloaders.append(DataLoader(Subset(valid_loader, idx), batch_size=batch_size, shuffle=True))\n"
      ],
      "metadata": {
        "id": "1o3JVlZUA4td"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"total train loaders:\", len(trainloaders))\n",
        "print(\"total  val loaders:\", len(valloaders))\n",
        "print(\"len of single train loader:\", len(trainloaders[0]))\n",
        "print(\"len of single val loader:\", len(valloaders[0]))\n",
        "print('feature size', feature_size)"
      ],
      "metadata": {
        "id": "7CMcGlBBb0Gq",
        "outputId": "0b11727e-314b-4277-dbc0-a0c5d3856b66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total train loaders: 5\n",
            "total  val loaders: 5\n",
            "len of single train loader: 13\n",
            "len of single val loader: 3\n",
            "feature size 307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = args.lr\n",
        "criterion = nn.SmoothL1Loss()"
      ],
      "metadata": {
        "id": "vbs8Cslbfc2X"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = fl.server.strategy.FedAvg(\n",
        "    fraction_fit=1.0,  # Sample 100% of available clients for training\n",
        "    fraction_evaluate=0.5,  # Sample 50% of available clients for evaluation\n",
        "    min_fit_clients=5,  # Never sample less than 10 clients for training\n",
        "    min_evaluate_clients=5,  # Never sample less than 5 clients for evaluation\n",
        "    min_available_clients=5,  # Wait until all 10 clients are available\n",
        "    evaluate_metrics_aggregation_fn =  weighted_average,\n",
        "    fit_metrics_aggregation_fn = fit_average\n",
        ")\n",
        "\n",
        "# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
        "client_resources = None\n",
        "if DEVICE.type == \"cuda\":\n",
        "    client_resources = {\"num_gpus\": 1}\n",
        "\n",
        "# Start simulation\n",
        "flower_history = fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=args.global_epochs),\n",
        "    strategy=strategy,\n",
        "    client_resources=client_resources,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Bait_DPYA4iI",
        "outputId": "75814b9f-114d-4ded-f59d-6715e51a35f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO flwr 2023-04-21 03:53:41,726 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=5, round_timeout=None)\n",
            "INFO:flwr:Starting Flower simulation, config: ServerConfig(num_rounds=5, round_timeout=None)\n",
            "2023-04-21 03:53:50,497\tINFO worker.py:1553 -- Started a local Ray instance.\n",
            "INFO flwr 2023-04-21 03:53:53,645 | app.py:180 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'node:172.28.0.12': 1.0, 'memory': 7709695182.0, 'object_store_memory': 3854847590.0, 'CPU': 2.0}\n",
            "INFO:flwr:Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'node:172.28.0.12': 1.0, 'memory': 7709695182.0, 'object_store_memory': 3854847590.0, 'CPU': 2.0}\n",
            "INFO flwr 2023-04-21 03:53:53,649 | server.py:86 | Initializing global parameters\n",
            "INFO:flwr:Initializing global parameters\n",
            "INFO flwr 2023-04-21 03:53:53,652 | server.py:273 | Requesting initial parameters from one random client\n",
            "INFO:flwr:Requesting initial parameters from one random client\n",
            "\u001b[2m\u001b[36m(pid=22903)\u001b[0m 2023-04-21 03:53:59.316753: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO flwr 2023-04-21 03:54:03,280 | server.py:277 | Received initial parameters from one random client\n",
            "INFO:flwr:Received initial parameters from one random client\n",
            "INFO flwr 2023-04-21 03:54:03,287 | server.py:88 | Evaluating initial parameters\n",
            "INFO:flwr:Evaluating initial parameters\n",
            "INFO flwr 2023-04-21 03:54:03,292 | server.py:101 | FL starting\n",
            "INFO:flwr:FL starting\n",
            "DEBUG flwr 2023-04-21 03:54:03,295 | server.py:218 | fit_round 1: strategy sampled 5 clients (out of 5)\n",
            "DEBUG:flwr:fit_round 1: strategy sampled 5 clients (out of 5)\n",
            "\u001b[2m\u001b[36m(pid=23002)\u001b[0m 2023-04-21 03:54:08.307059: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=23002)\u001b[0m training for epochs: 2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23002)\u001b[0m epoch#: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23002)\u001b[0m evaluating\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23002)\u001b[0m eval rmse loss:  151.5247521033654\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23002)\u001b[0m \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23002)\u001b[0m ##on train data## loss: 7.670723229646683, \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23002)\u001b[0m ##on train data## rmse loss: 151.5247521033654, mae loss: 115.90400226299579, mape loss: 186.9636609004094\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23002)\u001b[0m \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23002)\u001b[0m epoch#: 2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23002)\u001b[0m evaluating\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23002)\u001b[0m eval rmse loss:  131.12501115065353\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23002)\u001b[0m \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23002)\u001b[0m ##on train data## loss: 5.608185291290283, \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23002)\u001b[0m ##on train data## rmse loss: 131.12501115065353, mae loss: 105.06686166616586, mape loss: 181.90289093897894\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23002)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=23102)\u001b[0m 2023-04-21 03:54:21.283792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=23102)\u001b[0m training for epochs: 2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23102)\u001b[0m epoch#: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23102)\u001b[0m evaluating\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23102)\u001b[0m eval rmse loss:  162.9861778846154\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23102)\u001b[0m \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23102)\u001b[0m ##on train data## loss: 8.028758019208908, \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23102)\u001b[0m ##on train data## rmse loss: 162.9861778846154, mae loss: 125.2217548076923, mape loss: 184.85595675615164\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23102)\u001b[0m \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23102)\u001b[0m epoch#: 2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23102)\u001b[0m evaluating\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23102)\u001b[0m eval rmse loss:  151.15193997896634\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23102)\u001b[0m \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23102)\u001b[0m ##on train data## loss: 5.272757202386856, \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23102)\u001b[0m ##on train data## rmse loss: 151.15193997896634, mae loss: 112.37478579007663, mape loss: 140.42534232139587\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23102)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=23210)\u001b[0m 2023-04-21 03:54:36.156034: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=23210)\u001b[0m training for epochs: 2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23210)\u001b[0m epoch#: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23210)\u001b[0m evaluating\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23210)\u001b[0m eval rmse loss:  172.62841796875\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23210)\u001b[0m \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23210)\u001b[0m ##on train data## loss: 7.68116170167923, \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23210)\u001b[0m ##on train data## rmse loss: 172.62841796875, mae loss: 130.1878204345703, mape loss: 187.85671500059274\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23210)\u001b[0m \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23210)\u001b[0m epoch#: 2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23210)\u001b[0m evaluating\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23210)\u001b[0m eval rmse loss:  161.3770247239333\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23210)\u001b[0m \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23210)\u001b[0m ##on train data## loss: 6.517361283302307, \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23210)\u001b[0m ##on train data## rmse loss: 161.3770247239333, mae loss: 131.79827000544623, mape loss: 210.9935265320998\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23210)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=23314)\u001b[0m 2023-04-21 03:54:49.776325: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=23314)\u001b[0m training for epochs: 2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23314)\u001b[0m epoch#: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23314)\u001b[0m evaluating\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23314)\u001b[0m eval rmse loss:  164.18561495267429\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23314)\u001b[0m \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23314)\u001b[0m ##on train data## loss: 7.225103884935379, \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23314)\u001b[0m ##on train data## rmse loss: 164.18561495267429, mae loss: 123.87678821270282, mape loss: 143.55518176005438\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23314)\u001b[0m \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23314)\u001b[0m epoch#: 2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23314)\u001b[0m evaluating\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23314)\u001b[0m eval rmse loss:  220.17567091721756\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23314)\u001b[0m \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23314)\u001b[0m ##on train data## loss: 5.935123920440674, \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23314)\u001b[0m ##on train data## rmse loss: 220.17567091721756, mae loss: 181.236575786884, mape loss: 232.63172002939078\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23314)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=23427)\u001b[0m 2023-04-21 03:55:06.305315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=23427)\u001b[0m training for epochs: 2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23427)\u001b[0m epoch#: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23427)\u001b[0m evaluating\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23427)\u001b[0m eval rmse loss:  150.32128201998196\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23427)\u001b[0m \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23427)\u001b[0m ##on train data## loss: 7.101310759782791, \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23427)\u001b[0m ##on train data## rmse loss: 150.32128201998196, mae loss: 119.91652444692758, mape loss: 207.1232681091015\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23427)\u001b[0m \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23427)\u001b[0m epoch#: 2\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23427)\u001b[0m evaluating\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG flwr 2023-04-21 03:55:18,378 | server.py:232 | fit_round 1 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 1 received 5 results and 0 failures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=23427)\u001b[0m eval rmse loss:  246.510987501878\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23427)\u001b[0m \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23427)\u001b[0m ##on train data## loss: 7.678056210279465, \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23427)\u001b[0m ##on train data## rmse loss: 246.510987501878, mae loss: 195.53754366361179, mape loss: 304.50988457753107\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=23427)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG flwr 2023-04-21 03:55:18,805 | server.py:168 | evaluate_round 1: strategy sampled 5 clients (out of 5)\n",
            "DEBUG:flwr:evaluate_round 1: strategy sampled 5 clients (out of 5)\n",
            "\u001b[2m\u001b[36m(pid=23544)\u001b[0m 2023-04-21 03:55:23.885914: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=23544)\u001b[0m evaluating\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=23544)\u001b[0m eval rmse loss:  175.8794199625651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=23621)\u001b[0m 2023-04-21 03:55:30.712292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=23621)\u001b[0m evaluating\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=23621)\u001b[0m eval rmse loss:  174.39349365234375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=23688)\u001b[0m 2023-04-21 03:55:38.756041: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=23688)\u001b[0m evaluating\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=23688)\u001b[0m eval rmse loss:  171.62962849934897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=23768)\u001b[0m 2023-04-21 03:55:45.477742: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=23768)\u001b[0m evaluating\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=23768)\u001b[0m eval rmse loss:  172.91936747233072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=23833)\u001b[0m 2023-04-21 03:55:53.331377: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "with open('flower_history.pkl', 'wb') as f:\n",
        "    pickle.dump(flower_history, f)"
      ],
      "metadata": {
        "id": "1VlvzN1RA4f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('flower_history.pkl', 'rb') as f:\n",
        "    history = pickle.load(f)"
      ],
      "metadata": {
        "id": "IxuvVwntA4aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history of each client for training\n",
        "history.metrics_distributed_fit"
      ],
      "metadata": {
        "id": "ON6fZWjjA4Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history of each client for validation\n",
        "history.metrics_distributed"
      ],
      "metadata": {
        "id": "Px7mUAHFA4U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = [], []\n",
        "for average_metrics in history.metrics_distributed['average_metrics']:\n",
        "    x.append(average_metrics[0])\n",
        "    y.append(average_metrics[1]['valid_rmse'])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.plot(x, y, label='Average RMSE in each round')\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('RMSE')\n",
        "plt.legend()\n",
        "plt.title('Validation Curve for RMSE')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "deusxA6qA4SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x =  []\n",
        "y = [[] for i in history.metrics_distributed['client_metrics'][0][1]['valid_rmse']]\n",
        "for client_metrics in history.metrics_distributed['client_metrics']:\n",
        "    x.append(client_metrics[0])\n",
        "    for idx, client_loss in enumerate(client_metrics[1]['valid_rmse']):\n",
        "        y[idx].append(client_loss)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for client_num, loss_ in enumerate(y):\n",
        "    plt.plot(x, loss_, label='Validation RMSE of client: '+str(client_num))\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('RMSE')\n",
        "plt.legend()\n",
        "plt.title('Validation Curve for RMSE (individual client)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cuq_DrAzA4QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = [], []\n",
        "for average_metrics in history.metrics_distributed_fit['average_metrics']:\n",
        "    x.append(average_metrics[0])\n",
        "    y.append(average_metrics[1]['average loss'])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.plot(x, y, label='Average loss in each round')\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training Curve for Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PBeY1r6oA4M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x =  []\n",
        "y = [[] for i in history.metrics_distributed_fit['client_metrics'][0][1]['loss']]\n",
        "for client_metrics in history.metrics_distributed_fit['client_metrics']:\n",
        "    x.append(client_metrics[0])\n",
        "    for idx, client_loss in enumerate(client_metrics[1]['loss']):\n",
        "        y[idx].append(client_loss)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for client_num, loss_ in enumerate(y):\n",
        "    plt.plot(x, loss_, label='Training Loss of client: '+str(client_num))\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training Curve for Loss (individual client)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yCYs07d0CMIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "w0w_avUIkKJU"
      }
    }
  ]
}